{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6075232-0cda-443b-88bb-4a9b949aaa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\D-InfoTech\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">156,672</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ bidirectional_6 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │         \u001b[38;5;34m156,672\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_7 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m164,352\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_8 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m98,816\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">419,969</span> (1.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m419,969\u001b[0m (1.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">419,969</span> (1.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m419,969\u001b[0m (1.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 95ms/step - loss: 7774.5420 - mae: 77.3359 - val_loss: 6797.3472 - val_mae: 71.3167\n",
      "Epoch 2/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - loss: 5861.4092 - mae: 64.9034 - val_loss: 6285.1924 - val_mae: 68.1056\n",
      "Epoch 3/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - loss: 5500.5449 - mae: 62.3865 - val_loss: 5903.4424 - val_mae: 65.5037\n",
      "Epoch 4/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - loss: 5196.2886 - mae: 60.2554 - val_loss: 5560.7871 - val_mae: 63.1474\n",
      "Epoch 5/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 80ms/step - loss: 4838.9116 - mae: 57.5065 - val_loss: 5246.3042 - val_mae: 61.1077\n",
      "Epoch 6/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 83ms/step - loss: 4568.1924 - mae: 55.5118 - val_loss: 4944.1733 - val_mae: 58.5650\n",
      "Epoch 7/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 80ms/step - loss: 4336.0537 - mae: 53.6406 - val_loss: 4664.3711 - val_mae: 56.5655\n",
      "Epoch 8/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - loss: 4028.0886 - mae: 51.2055 - val_loss: 4399.7998 - val_mae: 54.6570\n",
      "Epoch 9/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 80ms/step - loss: 3801.7793 - mae: 49.5525 - val_loss: 4152.6729 - val_mae: 53.0869\n",
      "Epoch 10/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 80ms/step - loss: 3593.9744 - mae: 47.9222 - val_loss: 3912.6421 - val_mae: 51.0471\n",
      "Epoch 11/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - loss: 3356.4436 - mae: 45.8909 - val_loss: 3686.1055 - val_mae: 49.2890\n",
      "Epoch 12/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - loss: 3121.6970 - mae: 43.8450 - val_loss: 3472.5469 - val_mae: 47.8075\n",
      "Epoch 13/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 80ms/step - loss: 2978.0208 - mae: 42.7396 - val_loss: 3266.9282 - val_mae: 46.0537\n",
      "Epoch 14/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - loss: 2773.7673 - mae: 40.8737 - val_loss: 3073.0862 - val_mae: 44.7270\n",
      "Epoch 15/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 80ms/step - loss: 2636.1025 - mae: 39.9574 - val_loss: 2886.5146 - val_mae: 43.0699\n",
      "Epoch 16/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 80ms/step - loss: 2492.1648 - mae: 38.6086 - val_loss: 2709.3486 - val_mae: 41.4846\n",
      "Epoch 17/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - loss: 2307.0527 - mae: 36.9529 - val_loss: 2540.3445 - val_mae: 39.9854\n",
      "Epoch 18/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - loss: 2139.7959 - mae: 35.2745 - val_loss: 2380.3591 - val_mae: 38.5619\n",
      "Epoch 19/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - loss: 2017.5205 - mae: 34.1286 - val_loss: 2234.2061 - val_mae: 37.3550\n",
      "Epoch 20/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 96ms/step - loss: 1901.6511 - mae: 32.9826 - val_loss: 2083.0300 - val_mae: 35.8968\n",
      "Epoch 21/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - loss: 1743.2886 - mae: 31.3294 - val_loss: 1951.5890 - val_mae: 34.9389\n",
      "Epoch 22/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - loss: 1636.6151 - mae: 30.2555 - val_loss: 1815.5817 - val_mae: 33.3914\n",
      "Epoch 23/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - loss: 1545.0978 - mae: 29.5694 - val_loss: 1691.3066 - val_mae: 32.2813\n",
      "Epoch 24/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - loss: 1431.5427 - mae: 28.3911 - val_loss: 1576.3760 - val_mae: 31.0271\n",
      "Epoch 25/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 86ms/step - loss: 1314.5074 - mae: 26.9453 - val_loss: 1465.8453 - val_mae: 29.8578\n",
      "Epoch 26/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - loss: 1213.3167 - mae: 25.7549 - val_loss: 1371.2239 - val_mae: 29.2772\n",
      "Epoch 27/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - loss: 1138.0173 - mae: 25.0171 - val_loss: 1273.8661 - val_mae: 27.8990\n",
      "Epoch 28/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - loss: 1024.2653 - mae: 23.6238 - val_loss: 1178.4531 - val_mae: 26.9748\n",
      "Epoch 29/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - loss: 937.9631 - mae: 22.4945 - val_loss: 1088.3640 - val_mae: 25.7997\n",
      "Epoch 30/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - loss: 905.6002 - mae: 22.3363 - val_loss: 1009.3733 - val_mae: 25.0794\n",
      "Epoch 31/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - loss: 821.9446 - mae: 21.2778 - val_loss: 940.3401 - val_mae: 24.2607\n",
      "Epoch 32/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - loss: 748.5248 - mae: 20.2310 - val_loss: 867.4088 - val_mae: 23.4368\n",
      "Epoch 33/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - loss: 690.8818 - mae: 19.3876 - val_loss: 822.0702 - val_mae: 22.7042\n",
      "Epoch 34/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - loss: 639.0217 - mae: 18.7471 - val_loss: 762.2970 - val_mae: 22.2458\n",
      "Epoch 35/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 84ms/step - loss: 590.3329 - mae: 18.0484 - val_loss: 707.5362 - val_mae: 21.4224\n",
      "Epoch 36/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 84ms/step - loss: 531.1535 - mae: 17.0951 - val_loss: 652.9907 - val_mae: 20.8947\n",
      "Epoch 37/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - loss: 484.4004 - mae: 16.4009 - val_loss: 606.6927 - val_mae: 20.0431\n",
      "Epoch 38/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 83ms/step - loss: 447.2472 - mae: 15.7033 - val_loss: 580.9141 - val_mae: 19.5416\n",
      "Epoch 39/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 82ms/step - loss: 414.6094 - mae: 15.2102 - val_loss: 527.0269 - val_mae: 18.7735\n",
      "Epoch 40/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 85ms/step - loss: 371.3152 - mae: 14.4089 - val_loss: 513.7249 - val_mae: 18.1678\n",
      "Epoch 41/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 83ms/step - loss: 343.5616 - mae: 13.9166 - val_loss: 480.9702 - val_mae: 17.8496\n",
      "Epoch 42/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 84ms/step - loss: 321.0225 - mae: 13.4912 - val_loss: 441.2422 - val_mae: 17.2652\n",
      "Epoch 43/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 84ms/step - loss: 290.1574 - mae: 12.7913 - val_loss: 421.4124 - val_mae: 16.7727\n",
      "Epoch 44/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 86ms/step - loss: 264.4966 - mae: 12.3008 - val_loss: 415.2834 - val_mae: 16.3352\n",
      "Epoch 45/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 91ms/step - loss: 246.8139 - mae: 11.8701 - val_loss: 418.7378 - val_mae: 16.1290\n",
      "Epoch 46/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 89ms/step - loss: 232.6967 - mae: 11.6006 - val_loss: 368.2567 - val_mae: 15.5695\n",
      "Epoch 47/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - loss: 211.1507 - mae: 11.0457 - val_loss: 367.9437 - val_mae: 15.2592\n",
      "Epoch 48/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 89ms/step - loss: 192.5660 - mae: 10.4869 - val_loss: 350.0564 - val_mae: 14.7999\n",
      "Epoch 49/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - loss: 179.7414 - mae: 10.1501 - val_loss: 339.3638 - val_mae: 14.5407\n",
      "Epoch 50/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - loss: 170.3111 - mae: 9.8536 - val_loss: 350.8053 - val_mae: 14.4751\n",
      "Epoch 51/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 86ms/step - loss: 156.6239 - mae: 9.4021 - val_loss: 340.1592 - val_mae: 13.9785\n",
      "Epoch 52/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - loss: 151.9140 - mae: 9.2360 - val_loss: 305.3402 - val_mae: 13.4444\n",
      "Epoch 53/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 86ms/step - loss: 140.4373 - mae: 8.8338 - val_loss: 329.8178 - val_mae: 13.8065\n",
      "Epoch 54/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - loss: 134.0330 - mae: 8.5792 - val_loss: 341.4183 - val_mae: 13.7800\n",
      "Epoch 55/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 86ms/step - loss: 131.6769 - mae: 8.5027 - val_loss: 328.6742 - val_mae: 13.3019\n",
      "Epoch 56/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - loss: 123.5432 - mae: 8.1839 - val_loss: 321.3815 - val_mae: 13.1753\n",
      "Epoch 57/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - loss: 117.7493 - mae: 7.9779 - val_loss: 335.1362 - val_mae: 13.4203\n",
      "Epoch 58/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - loss: 111.7298 - mae: 7.7165 - val_loss: 317.1355 - val_mae: 12.9649\n",
      "Epoch 59/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 89ms/step - loss: 109.1413 - mae: 7.6187 - val_loss: 298.4217 - val_mae: 12.4910\n",
      "Epoch 60/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 91ms/step - loss: 102.8020 - mae: 7.3861 - val_loss: 320.4681 - val_mae: 12.6960\n",
      "Epoch 61/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 110ms/step - loss: 94.9322 - mae: 7.1750 - val_loss: 309.5586 - val_mae: 12.5686\n",
      "Epoch 62/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 116ms/step - loss: 89.8815 - mae: 6.8932 - val_loss: 320.5987 - val_mae: 12.6906\n",
      "Epoch 63/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 105ms/step - loss: 84.7344 - mae: 6.7162 - val_loss: 325.4436 - val_mae: 12.6222\n",
      "Epoch 64/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 91ms/step - loss: 90.8520 - mae: 6.8792 - val_loss: 309.5186 - val_mae: 12.3407\n",
      "Epoch 65/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - loss: 76.3802 - mae: 6.3265 - val_loss: 315.4513 - val_mae: 12.5953\n",
      "Epoch 66/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 89ms/step - loss: 76.1034 - mae: 6.3714 - val_loss: 301.5591 - val_mae: 12.2607\n",
      "Epoch 67/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 90ms/step - loss: 74.4973 - mae: 6.2754 - val_loss: 311.5573 - val_mae: 12.3213\n",
      "Epoch 68/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 97ms/step - loss: 73.2277 - mae: 6.2335 - val_loss: 305.6201 - val_mae: 12.0264\n",
      "Epoch 69/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 104ms/step - loss: 73.6525 - mae: 6.2534 - val_loss: 337.5414 - val_mae: 12.8174\n",
      "Epoch 70/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - loss: 65.7671 - mae: 5.9386 - val_loss: 353.7365 - val_mae: 13.0129\n",
      "Epoch 71/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - loss: 69.6274 - mae: 6.0472 - val_loss: 318.2987 - val_mae: 12.4943\n",
      "Epoch 72/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 92ms/step - loss: 64.1960 - mae: 5.8899 - val_loss: 319.3165 - val_mae: 12.4568\n",
      "Epoch 73/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 92ms/step - loss: 63.0392 - mae: 5.8417 - val_loss: 327.1420 - val_mae: 12.2478\n",
      "Epoch 74/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 91ms/step - loss: 59.6986 - mae: 5.6951 - val_loss: 343.2129 - val_mae: 12.8057\n",
      "Epoch 75/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 89ms/step - loss: 59.2606 - mae: 5.7176 - val_loss: 363.5563 - val_mae: 13.1761\n",
      "Epoch 76/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - loss: 58.7044 - mae: 5.6396 - val_loss: 339.1463 - val_mae: 12.5441\n",
      "Epoch 77/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 76ms/step - loss: 56.5001 - mae: 5.5790 - val_loss: 317.7529 - val_mae: 12.3822\n",
      "Epoch 78/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 76ms/step - loss: 57.0359 - mae: 5.6354 - val_loss: 346.2216 - val_mae: 12.8988\n",
      "Epoch 79/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 75ms/step - loss: 54.5896 - mae: 5.4693 - val_loss: 352.3040 - val_mae: 12.9672\n",
      "Epoch 80/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 76ms/step - loss: 54.3543 - mae: 5.4723 - val_loss: 361.9690 - val_mae: 13.1124\n",
      "Epoch 81/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 75ms/step - loss: 52.5189 - mae: 5.3995 - val_loss: 362.2742 - val_mae: 13.1756\n",
      "Epoch 82/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - loss: 51.6905 - mae: 5.3509 - val_loss: 347.2109 - val_mae: 12.8330\n",
      "Epoch 83/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 77ms/step - loss: 52.0476 - mae: 5.3400 - val_loss: 378.1831 - val_mae: 13.5063\n",
      "Epoch 84/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 80ms/step - loss: 56.4596 - mae: 5.5355 - val_loss: 340.0422 - val_mae: 12.7650\n",
      "Epoch 85/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - loss: 48.3129 - mae: 5.1962 - val_loss: 331.8967 - val_mae: 12.5798\n",
      "Epoch 86/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - loss: 52.2863 - mae: 5.3683 - val_loss: 341.2024 - val_mae: 12.6228\n",
      "Epoch 87/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - loss: 47.2605 - mae: 5.1407 - val_loss: 346.0582 - val_mae: 12.8326\n",
      "Epoch 88/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - loss: 47.0469 - mae: 5.1521 - val_loss: 339.3871 - val_mae: 12.6621\n",
      "Epoch 89/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - loss: 48.7327 - mae: 5.2185 - val_loss: 339.5708 - val_mae: 12.5502\n",
      "Epoch 90/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - loss: 47.5174 - mae: 5.1482 - val_loss: 375.0400 - val_mae: 12.9650\n",
      "Epoch 91/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 80ms/step - loss: 46.4020 - mae: 5.1019 - val_loss: 358.0351 - val_mae: 13.1500\n",
      "Epoch 92/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 80ms/step - loss: 45.0686 - mae: 5.0408 - val_loss: 341.8941 - val_mae: 12.4578\n",
      "Epoch 93/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - loss: 46.4064 - mae: 5.0775 - val_loss: 343.9295 - val_mae: 12.4512\n",
      "Epoch 94/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - loss: 44.8450 - mae: 5.0276 - val_loss: 354.1220 - val_mae: 12.9821\n",
      "Epoch 95/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - loss: 43.8840 - mae: 4.9738 - val_loss: 334.0592 - val_mae: 12.5481\n",
      "Epoch 96/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - loss: 45.3317 - mae: 5.0513 - val_loss: 355.2768 - val_mae: 12.9574\n",
      "Epoch 97/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - loss: 43.2093 - mae: 4.9001 - val_loss: 368.2710 - val_mae: 13.2134\n",
      "Epoch 98/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 78ms/step - loss: 45.2389 - mae: 5.0279 - val_loss: 359.2920 - val_mae: 13.0649\n",
      "Epoch 99/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 79ms/step - loss: 45.9731 - mae: 5.0282 - val_loss: 339.7422 - val_mae: 12.5350\n",
      "Epoch 100/100\n",
      "\u001b[1m207/207\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 81ms/step - loss: 43.5033 - mae: 4.9208 - val_loss: 349.8053 - val_mae: 12.8269\n",
      "LSTM-Bidirectional Training Time: 1757.28 seconds\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001F381C16340> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001F381C16340> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 360ms/step\n",
      "LSTM-Bidirectional Model Results:\n",
      "  RMSE     = 374.69\n",
      "  MAE      = 14.49\n",
      "  R²       = 0.78\n",
      "  Accuracy = 77.79%\n",
      "  NASA Score = 882.01\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Imports -------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ------------------- Load Data -------------------\n",
    "column_names = ['engine_id', 'cycle', 'op_setting_1', 'op_setting_2', 'op_setting_3'] + [f'sensor_{i}' for i in range(1, 22)]\n",
    "train_df = pd.read_csv('train_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "test_df = pd.read_csv('test_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "rul_df = pd.read_csv('RUL_FD001.txt', header=None, names=[\"true_RUL\"])\n",
    "rul_df[\"engine_id\"] = rul_df.index + 1\n",
    "\n",
    "# ------------------- Preprocessing -------------------\n",
    "# Drop sensor_22 and sensor_23 if present (optional)\n",
    "train_df.drop(columns=[\"sensor_22\", \"sensor_23\"], errors='ignore', inplace=True)\n",
    "test_df.drop(columns=[\"sensor_22\", \"sensor_23\"], errors='ignore', inplace=True)\n",
    "\n",
    "# Define features\n",
    "features = train_df.columns.difference(['engine_id', 'cycle', 'RUL'])\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "train_df[features] = scaler.fit_transform(train_df[features])\n",
    "test_df[features] = scaler.transform(test_df[features])\n",
    "\n",
    "# Compute RUL\n",
    "train_max_cycle = train_df.groupby('engine_id')['cycle'].max().reset_index()\n",
    "train_max_cycle.columns = ['engine_id', 'max_cycle']\n",
    "train_df = train_df.merge(train_max_cycle, on='engine_id')\n",
    "train_df['RUL'] = train_df['max_cycle'] - train_df['cycle']\n",
    "train_df.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "test_max_cycle = test_df.groupby('engine_id')['cycle'].max().reset_index()\n",
    "test_max_cycle.columns = ['engine_id', 'max_cycle']\n",
    "test_df = test_df.merge(test_max_cycle, on='engine_id')\n",
    "test_df['RUL'] = test_df['max_cycle'] - test_df['cycle']\n",
    "test_df.drop('max_cycle', axis=1, inplace=True)\n",
    "\n",
    "# Clip RUL\n",
    "rul_cap = 130\n",
    "train_df['RUL'] = train_df['RUL'].clip(upper=rul_cap)\n",
    "\n",
    "# ------------------- Sequence Creation -------------------\n",
    "sequence_length = 30\n",
    "\n",
    "def create_sequences(df, sequence_length, features):\n",
    "    sequences, labels = [], []\n",
    "    for engine_id in df['engine_id'].unique():\n",
    "        engine_data = df[df['engine_id'] == engine_id]\n",
    "        for i in range(len(engine_data) - sequence_length):\n",
    "            seq = engine_data[features].iloc[i:i+sequence_length].values\n",
    "            label = engine_data['RUL'].iloc[i + sequence_length]\n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(train_df, sequence_length, features)\n",
    "num_features = X_train_seq.shape[2]\n",
    "\n",
    "# ------------------- LSTM-Bidirectional Model -------------------\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(sequence_length, num_features)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "# ------------------- Training -------------------\n",
    "start_time = time.time()\n",
    "model.fit(X_train_seq, y_train_seq, validation_split=0.25, epochs=100, batch_size=64)\n",
    "end_time = time.time()\n",
    "print(f\"LSTM-Bidirectional Training Time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# ------------------- Prepare Test Data -------------------\n",
    "test_sequences = []\n",
    "valid_engine_ids = []\n",
    "\n",
    "for engine_id in test_df[\"engine_id\"].unique():\n",
    "    engine_data = test_df[test_df[\"engine_id\"] == engine_id]\n",
    "    if len(engine_data) >= sequence_length:\n",
    "        last_seq = engine_data.iloc[-sequence_length:][features].values\n",
    "        test_sequences.append(last_seq)\n",
    "        valid_engine_ids.append(engine_id)\n",
    "\n",
    "X_test_final = np.array(test_sequences)\n",
    "X_test_final = np.nan_to_num(X_test_final)  # Ensure no NaNs\n",
    "\n",
    "y_pred = model.predict(X_test_final).flatten()\n",
    "y_true = rul_df[rul_df[\"engine_id\"].isin(valid_engine_ids)][\"true_RUL\"].values\n",
    "\n",
    "# ------------------- Evaluation -------------------\n",
    "def evaluate_model(name, y_true, y_pred):\n",
    "    rmse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    accuracy = 100 - mape\n",
    "    h = y_pred - y_true\n",
    "    score = np.sum(np.where(h < 0, np.exp(-h / 13) - 1, np.exp(h / 10) - 1))\n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  RMSE     = {rmse:.2f}\")\n",
    "    print(f\"  MAE      = {mae:.2f}\")\n",
    "    print(f\"  R²       = {r2:.2f}\")\n",
    "    print(f\"  Accuracy = {accuracy:.2f}%\")\n",
    "    print(f\"  NASA Score = {score:.2f}\")\n",
    "\n",
    "evaluate_model(\"LSTM-Bidirectional Model\", y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0976ab-b036-4fa5-b66b-65720f8b512c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (TF)",
   "language": "python",
   "name": "tf311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
